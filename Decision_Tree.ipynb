{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "TlcECrUvap1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "   classification?\n",
        "   - A Decision Tree is a supervised learning algorithm used for classification (and also regression). It works by splitting the data into branches based on feature values, forming a tree-like structure.\n",
        "\n",
        "   - In classification, the tree starts with a root node and asks simple yes/no questions (conditions). Based on the answer, the data moves down a branch to another node. This process continues until it reaches a leaf node, which gives the final class label.\n",
        "\n",
        "   - In simple words, a decision tree makes decisions step by step, just like a flowchart, to classify data into different categories.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "   How do they impact the splits in a Decision Tree?\n",
        "   - Gini Impurity and Entropy are measures used in decision trees to check how impure a node is (how mixed the classes are).\n",
        "\n",
        "   - Gini Impurity measures the chance of wrongly classifying a data point if it is randomly labeled. A Gini value of 0 means the node is pure (only one class). Lower Gini is better.\n",
        "\n",
        "   - Entropy measures the level of disorder or uncertainty in a node. Entropy is 0 when all data belongs to one class and higher when classes are mixed.\n",
        "\n",
        "   - Impact on splits : Decision Trees choose the split that reduces Gini Impurity or Entropy the most, making child nodes purer. Better splits create clearer class separation.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "   Trees? Give one practical advantage of using each.\n",
        "   - Pre-Pruning stops the decision tree from growing early, by setting limits like maximum depth or minimum samples per node.\n",
        "   - Advantage: It reduces overfitting and saves training time.\n",
        "\n",
        "   - Post-Pruning allows the tree to grow fully and then cuts back unnecessary branches.\n",
        "   - Advantage: It often gives better accuracy by removing branches that do not improve performance.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "   choosing the best split?\n",
        "   - Information Gain is a measure used in decision trees to decide the best feature for splitting the data.\n",
        "\n",
        "   - It shows how much entropy (uncertainty) is reduced after a split. A higher information gain means the split creates purer child nodes.\n",
        "\n",
        "   - Importance : Decision Trees choose the split with the highest Information Gain because it separates the data more clearly, leading to better classification.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "   what are their main advantages and limitations?\n",
        "   - Real-world applications of Decision Trees:\n",
        "     - Decision Trees are used in credit risk analysis (loan approval), medical diagnosis (disease detection), customer segmentation in marketing, fraud detection, and spam email classification.\n",
        "\n",
        "   - Advantages:\n",
        "     - They are easy to understand and interpret, work with both numerical and categorical data, and require little data preprocessing.\n",
        "\n",
        "   - Limitations:\n",
        "     - Decision Trees can overfit the data, are sensitive to small changes in data, and may give lower accuracy compared to ensemble models.\n"
      ],
      "metadata": {
        "id": "KTWIp1EKayLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(feature, \":\", importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCMqYZDcmRx",
        "outputId": "0aef0e64-0ca8-4e1e-effc-d12cc7b22bbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm) : 0.0\n",
            "sepal width (cm) : 0.016670139612419255\n",
            "petal length (cm) : 0.9061433868879218\n",
            "petal width (cm) : 0.07718647349965893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to:\n",
        "#  Load the Iris Dataset\n",
        "#  Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_limited.fit(X_train, y_train)\n",
        "y_pred_limited = dt_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", acc_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", acc_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YICXYE4XdVdm",
        "outputId": "d75fee02-af99-486c-abc3-e5b803857085"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to:\n",
        "# Load the Boston Housing Dataset\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df = pd.read_csv(\"boston.csv\")\n",
        "\n",
        "X = df.drop(\"MEDV\", axis=1)\n",
        "y = df[\"MEDV\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, model.feature_importances_):\n",
        "    print(feature, \":\", importance)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ZYKSBkd4X9",
        "outputId": "e69b7726-2089-459f-b4f8-585897c8341c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.416078431372549\n",
            "\n",
            "Feature Importances:\n",
            "CRIM : 0.05129567385985361\n",
            "ZN : 0.003352705854613196\n",
            "INDUS : 0.005816191711420081\n",
            "CHAS : 2.279406506977855e-06\n",
            "NOX : 0.027148378971777364\n",
            "RM : 0.6003262563803439\n",
            "AGE : 0.01361706300057042\n",
            "DIS : 0.07068816216312718\n",
            "RAD : 0.0019406229702577159\n",
            "TAX : 0.012463865318667847\n",
            "PTRATIO : 0.011011608907585924\n",
            "B : 0.00900872741695864\n",
            "LSTAT : 0.1933284640383171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to:\n",
        "#  Load the Iris Dataset\n",
        "#  Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#  Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LfjZUAbgnUW",
        "outputId": "0c6f360d-8d56-4974-9e7e-8334b3632873"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "     wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "     Explain the step-by-step process you would follow to:\n",
        "      - Handle the missing values\n",
        "      - Encode the categorical features\n",
        "      - Train a Decision Tree model\n",
        "      - Tune its hyperparameters\n",
        "      - Evaluate its performance\n",
        "      - And describe what business value this model could provide in the\n",
        "        real-world setting.\n",
        "\n",
        "   - As a data scientist, I would follow these steps:\n",
        "\n",
        "     - First, I would handle missing values by filling numerical columns with the mean or median and categorical columns with the most frequent value. If a column has too many missing values and is not useful, I may remove it.\n",
        "\n",
        "     - Next, I would encode categorical features so the model can understand them. For nominal categories (like gender or city), I would use One-Hot Encoding. For ordinal categories (like disease stage), I would use Label Encoding.\n",
        "\n",
        "     - Then, I would train a Decision Tree model by splitting the data into training and testing sets and fitting a Decision Tree classifier on the training data.\n",
        "\n",
        "     - After that, I would tune hyperparameters such as max_depth, min_samples_split, and min_samples_leaf using GridSearchCV to reduce overfitting and improve accuracy.\n",
        "\n",
        "     - To evaluate the model, I would check metrics like accuracy, precision, recall, F1-score, and the confusion matrix, because in healthcare false negatives are very costly.\n",
        "\n",
        "     - Business value:\n",
        "       - This model can help doctors identify high-risk patients early, support faster diagnosis, reduce human error, and improve patient outcomes. It also helps hospitals optimize resources and lower treatment costs by enabling early intervention."
      ],
      "metadata": {
        "id": "dAX_itw_htHs"
      }
    }
  ]
}